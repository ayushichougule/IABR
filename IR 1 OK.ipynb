{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5cf79c1-8e2a-4663-ae6c-51a114306cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed_text: artifici intellig ai transform variou industri healthcar financ transport entertain system capabl perform complex task diagnos diseas predict stock market trend even drive autonom vehicl machin learn subset ai enabl comput learn data improv perform time larg dataset power comput tool machin learn model train recogn pattern make predict provid recommend howev grow influenc ai also rais import ethic concern machin becom intellig question aris privaci job displac potenti bia ai algorithm ongo debat ensur ai develop use respons mani call regul govern use balanc technolog innov societ essenti futur ai nevertheless potenti benefit ai immens har properli revolution live work solv world difficult problem process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\AYUSHI\n",
      "[nltk_data]     CHOUGULE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\AYUSHI\n",
      "[nltk_data]     CHOUGULE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "file_path='sample.txt'\n",
    "\n",
    "def preprocess_text_file(file_path):\n",
    "\n",
    "    #read the text file\n",
    "    with open(file_path,'r',encoding='utf-8')as file:\n",
    "        text=file.read()\n",
    "\n",
    "    #tokenization\n",
    "    tokens=word_tokenize(text)\n",
    "\n",
    "    tokens=[word.lower() for word in tokens]\n",
    "\n",
    "    #stopword removal\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filtered_tokens=[word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    #stemming\n",
    "    stemmer=PorterStemmer()\n",
    "    stemmed_tokens=[stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "    processed_text=' '.join(stemmed_tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "processed_text=preprocess_text_file(file_path)\n",
    "print(\"Processed_text:\",processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d960664-8930-4a72-a772-4527b7ef3c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
